{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca37fcf2",
   "metadata": {},
   "source": [
    "## Generate the Explainability Output\n",
    "\n",
    "In this notebook, we compute the explainability output used in the paper. The methods include:\n",
    "1. Accumulated Local Effects (ALE)  \n",
    "2. SHAP (Shapley Additive Explanations)\n",
    "3. SAGE (Shapley Additive Global Explanations)\n",
    "4. Grouped SAGE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1cd95c",
   "metadata": {},
   "source": [
    "#### Import python packages (internal and third party)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c329e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neccesary packages to load the ML models from pickle\n",
    "%pip install scikit-explain==0.1.4 sage-importance imblearn daal4py scikit-learn==1.0.2 netCDF4 scikit-learn-intelex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba4f8bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n"
     ]
    }
   ],
   "source": [
    "# Append the explain_tutorial_repo to the system path\n",
    "import sys, os \n",
    "from os.path import dirname\n",
    "path = dirname(dirname(os.getcwd()))\n",
    "sys.path.insert(0, path)\n",
    "#sys.path.insert(0, '/home/monte.flora/python_packages/scikit-explain')\n",
    "\n",
    "import skexplain \n",
    "from skexplain.common.importance_utils import to_skexplain_importance\n",
    "from src.io.io import load_data_and_model\n",
    "from src.common.util import subsampler, normalize_importance, compute_sage\n",
    "\n",
    "import pickle\n",
    "import shap\n",
    "import itertools\n",
    "import numpy as np\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d212d106",
   "metadata": {},
   "source": [
    "#### Setting the user constants (paths, parameters, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ba84d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants. \n",
    "N_BOOTSTRAP = 10\n",
    "N_BINS = 30\n",
    "N_JOBS = 4\n",
    "GLOBAL_SIZE = 5\n",
    "LOCAL_SIZE = 5\n",
    "DATASET = 'road_surface'\n",
    "\n",
    "EVALUATION_FN = 'norm_aupdc'\n",
    "\n",
    "if not os.path.exists('results'):\n",
    "    os.mkdir('results')\n",
    "    \n",
    "BASE_PATH       = os.getcwd()\n",
    "RESULTS_PATH    = os.path.join(BASE_PATH, 'results')\n",
    "DATA_BASE_PATH  = os.path.join(BASE_PATH, 'datasets')\n",
    "MODEL_BASE_PATH = os.path.join(BASE_PATH, 'models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7a6bf9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function compute_sage_ at 0x7fe5bcb04b80>\n",
      "PermutationEstimator will use 4 jobs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22338a48da0e4e018fd9e52ac02c5999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function compute_group_sage at 0x7fe5c181e310>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ba77e1796f341348678fc7e2ead0ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute ALE \n",
    "def compute_ale(explainer, dataset, est_name, **kwargs): \n",
    "    ale = explainer.ale(features='all', n_bootstrap=N_BOOTSTRAP, n_bins=N_BINS, n_jobs=N_JOBS)\n",
    "    # Save ALE results (as netcdf file)\n",
    "    explainer.save(os.path.join(RESULTS_PATH, f'ale_{dataset}.nc'), ale, encoding=None)\n",
    "\n",
    "\n",
    "# Compute Shapely Additive Explanation (SHAP)\n",
    "def compute_shap(explainer, dataset, est_name, **kwargs):\n",
    "    X = kwargs['X']\n",
    "    features = kwargs['X'].columns\n",
    "    results = explainer.local_attributions('shap', \n",
    "                                       shap_kws={'masker' : \n",
    "                                      shap.maskers.Partition(X, max_samples=50, \n",
    "                                                             clustering=\"correlation\"), \n",
    "                                     'algorithm' : 'permutation'})\n",
    "\n",
    "\n",
    "    shap_rank = to_skexplain_importance(results[f'shap_values__{est_name}'].values, \n",
    "                                     estimator_name=est_name, \n",
    "                                     feature_names=features, \n",
    "                                     method ='shap_sum', \n",
    "                                     normalize=False    \n",
    "                                       )\n",
    "\n",
    "    # Sum the SHAP values for each feature and then save results. \n",
    "    explainer.save(os.path.join(RESULTS_PATH, f'shap_{dataset}.nc'), results, encoding=None)\n",
    "    explainer.save(os.path.join(RESULTS_PATH, f'shap_rank_{dataset}.nc'), shap_rank, encoding=None)\n",
    "\n",
    "\n",
    "# ## 7. SAGE Values\n",
    "# Compute SAGE\n",
    "def compute_sage_(explainer, dataset, est_name, **kwargs):\n",
    "    estimator = explainer.estimators[est_name]\n",
    "    \n",
    "    X = explainer.X\n",
    "    y = explainer.y\n",
    "    X_orig = kwargs['X']\n",
    "    \n",
    "    features = kwargs['X'].columns\n",
    "    sage_values = compute_sage(estimator, X.values, y, X_orig, n_jobs = N_JOBS)\n",
    "    sage_rank = to_skexplain_importance(sage_values,\n",
    "                                     estimator_name=est_name, \n",
    "                                     feature_names=features, \n",
    "                                     method = 'sage', \n",
    "                                     normalize=False  \n",
    "                                       )\n",
    "\n",
    "    # Sum the SAGE values for each feature and then save results. \n",
    "    explainer.save(os.path.join(RESULTS_PATH, f'sage_{dataset}.nc'), sage_rank, encoding=None)\n",
    "\n",
    "\n",
    "# Compute Grouped SAGE\n",
    "def compute_group_sage(explainer, dataset,  est_name, **kwargs):\n",
    "    \n",
    "    X = explainer.X\n",
    "    feature_groups = kwargs['groups']\n",
    "    # Group indices\n",
    "    groups = []\n",
    "    cols = list(X.columns)\n",
    "    features = []\n",
    "    for key, group in feature_groups.items():\n",
    "        ind_list = []\n",
    "        for feature in group:\n",
    "            ind_list.append(cols.index(feature))\n",
    "        groups.append(ind_list)\n",
    "        features.append(key)  \n",
    "    \n",
    "    estimator = explainer.estimators[est_name]\n",
    "    \n",
    "    y = explainer.y\n",
    "    X_orig = kwargs['X']\n",
    "    \n",
    "    sage_values = compute_sage(estimator, X.values, y, X_orig, groups=groups)\n",
    "    sage_rank = to_skexplain_importance(sage_values,\n",
    "                                     estimator_name=est_name, \n",
    "                                     feature_names=features, \n",
    "                                     method = 'sage', \n",
    "                                     normalize=False  \n",
    "                                       )\n",
    "\n",
    "    # Sum the SAGE values for each feature and then save results. \n",
    "    explainer.save(os.path.join(RESULTS_PATH, f'grouped_sage_{dataset}.nc'), sage_rank, encoding=None)\n",
    "    \n",
    "# For these tutorial notebooks, we explore the road surface dataset.\n",
    "# Feel free to set 'severe_wind' or 'lightning' to explore the other datasets!\n",
    "    \n",
    "global_methods = [compute_ale, compute_sage_, compute_group_sage,]\n",
    "local_methods = [compute_shap]\n",
    "\n",
    "# Load model and data.\n",
    "model, X, y, groups = load_data_and_model(DATASET, DATA_BASE_PATH, MODEL_BASE_PATH, \n",
    "                                     return_groups=True)\n",
    "    \n",
    "est_name = model[0]\n",
    "    \n",
    "# Subsample the dataset with GLOBAL_SIZE samples for the global methods. \n",
    "X_sub, y_sub = subsampler(X,y, GLOBAL_SIZE)\n",
    "y_sub = np.array([0,1,0,0,1])\n",
    "\n",
    "# Initialize the explainer. \n",
    "global_explainer = skexplain.ExplainToolkit(model, X_sub, y_sub) \n",
    "    \n",
    "# Subsample the GLOBAL_SIZE samples with LOCAL_SIZE samples for the local methods\n",
    "X_local, y_local = subsampler(X_sub, y_sub, LOCAL_SIZE)\n",
    "local_explainer = skexplain.ExplainToolkit(model, X_local, y_local)\n",
    "    \n",
    "for method in local_methods:\n",
    "    print(method)\n",
    "    method(local_explainer, dataset, est_name, X=X)\n",
    "\n",
    "for method in global_methods:\n",
    "    print(method)\n",
    "    method(global_explainer, dataset, est_name, X=X, model=model, groups=groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4391618e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
