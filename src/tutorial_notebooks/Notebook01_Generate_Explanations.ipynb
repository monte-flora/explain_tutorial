{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca37fcf2",
   "metadata": {},
   "source": [
    "## Generate the Explainability Output\n",
    "\n",
    "In this notebook, we compute the explainability output used in the paper. The methods include:\n",
    "1. Accumulated Local Effects (ALE)  \n",
    "2. SHAP (Shapley Additive Explanations)\n",
    "3. SAGE (Shapley Additive Global Explanations)\n",
    "4. Grouped SAGE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1cd95c",
   "metadata": {},
   "source": [
    "#### Import python packages (internal and third party)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba4f8bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os \n",
    "from os.path import dirname\n",
    "path = dirname(dirname(os.getcwd()))\n",
    "sys.path.insert(0, path)\n",
    "sys.path.insert(0, '/home/monte.flora/python_packages/scikit-explain')\n",
    "\n",
    "import skexplain \n",
    "from skexplain.common.importance_utils import to_skexplain_importance\n",
    "from src.io.io import load_data_and_model\n",
    "from src.common.util import subsampler, normalize_importance, compute_sage\n",
    "\n",
    "import pickle\n",
    "import shap\n",
    "import itertools\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d212d106",
   "metadata": {},
   "source": [
    "#### Setting the user constants (paths, parameters, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba84d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants. \n",
    "N_BOOTSTRAP = 10\n",
    "N_BINS = 30\n",
    "EVALUATION_FN = 'norm_aupdc'\n",
    "\n",
    "BASE_PATH = '/work/mflora/explainability_work/'\n",
    "\n",
    "RESULTS_PATH = os.path.join(BASE_PATH,    'results')\n",
    "DATA_BASE_PATH = os.path.join(BASE_PATH,  'datasets')\n",
    "MODEL_BASE_PATH = os.path.join(BASE_PATH, 'models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7a6bf9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61426, 91)\n",
      "<function compute_group_sage at 0x1459168235e0>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d485b9e200494c96f807f3b6849a9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute ALE \n",
    "def compute_ale(explainer, dataset, option, est_name, **kwargs):\n",
    "    features = kwargs['X'].columns\n",
    "    n_jobs = len(X.columns)\n",
    "    ale = explainer.ale(features='all', n_bootstrap=N_BOOTSTRAP, n_bins=N_BINS, n_jobs=n_jobs)\n",
    "    # Save the raw ALE and ALE-variance rankings results for paper 1 \n",
    "    explainer.save(os.path.join(RESULTS_PATH, f'ale_{dataset}_{option}.nc'), ale)\n",
    "\n",
    "\n",
    "# ## 6. SHAP Values\n",
    "# Compute SHAP (Approx. Owen Values)\n",
    "# The default explainer is the PermutationExplainer. The PermutationExplainer uses a \n",
    "# simple forward- and backward-permutation scheme to compute the SHAP values. \n",
    "# The SHAP documentation claims this method is exact for 2nd order interactions.\n",
    "# For the maskers, we are using correlations and as such we are computing \n",
    "# approximate Owen values. \n",
    "\n",
    "# Check if each SHAP example can be ran in parallel. \n",
    "def compute_shap(explainer, dataset, option, est_name, **kwargs):\n",
    "    X = kwargs['X']\n",
    "    features = kwargs['X'].columns\n",
    "    results = explainer.local_attributions('shap', \n",
    "                                       shap_kws={'masker' : \n",
    "                                      shap.maskers.Partition(X, max_samples=50, \n",
    "                                                             clustering=\"correlation\"), \n",
    "                                     'algorithm' : 'permutation'})\n",
    "\n",
    "\n",
    "    shap_rank = to_skexplain_importance(results[f'shap_values__{est_name}'].values, \n",
    "                                     estimator_name=est_name, \n",
    "                                     feature_names=features, \n",
    "                                     method ='shap_sum', \n",
    "                                     normalize=False    \n",
    "                                       )\n",
    "\n",
    "    # Sum the SHAP values for each feature and then save results. \n",
    "    explainer.save(os.path.join(RESULTS_PATH, f'shap_{dataset}_{option}.nc'), results)\n",
    "    explainer.save(os.path.join(RESULTS_PATH, f'shap_rank_{dataset}_{option}.nc'), shap_rank)\n",
    "\n",
    "\n",
    "# ## 7. SAGE Values\n",
    "# Compute SAGE\n",
    "def compute_sage_(explainer, dataset, option, est_name, **kwargs):\n",
    "    estimator = explainer.estimators[est_name]\n",
    "    \n",
    "    X = explainer.X\n",
    "    y = explainer.y\n",
    "    X_orig = kwargs['X']\n",
    "    \n",
    "    features = kwargs['X'].columns\n",
    "    sage_values = compute_sage(estimator, X.values, y, X_orig, n_jobs = X.shape[1])\n",
    "    sage_rank = to_skexplain_importance(sage_values,\n",
    "                                     estimator_name=est_name, \n",
    "                                     feature_names=features, \n",
    "                                     method = 'sage', \n",
    "                                     normalize=False  \n",
    "                                       )\n",
    "\n",
    "    # Sum the SAGE values for each feature and then save results. \n",
    "    explainer.save(os.path.join(RESULTS_PATH, f'sage_{dataset}_{option}.nc'), sage_values)\n",
    "    #explainer.save(os.path.join(RESULTS_PATH, f'new_sage_rank_{dataset}_{option}.nc'), sage_rank)\n",
    "\n",
    "\n",
    "# Compute Grouped SAGE\n",
    "def compute_group_sage(explainer, dataset, option, est_name, **kwargs):\n",
    "    \n",
    "    X = explainer.X\n",
    "    feature_groups = kwargs['groups']\n",
    "    # Group indices\n",
    "    groups = []\n",
    "    cols = list(X.columns)\n",
    "    features = []\n",
    "    for key, group in feature_groups.items():\n",
    "        ind_list = []\n",
    "        for feature in group:\n",
    "            ind_list.append(cols.index(feature))\n",
    "        groups.append(ind_list)\n",
    "        features.append(key)  \n",
    "    \n",
    "    estimator = explainer.estimators[est_name]\n",
    "    \n",
    "    y = explainer.y\n",
    "    X_orig = kwargs['X']\n",
    "    \n",
    "    sage_values = compute_sage(estimator, X.values, y, X_orig, groups=groups)\n",
    "    sage_rank = to_skexplain_importance(sage_values,\n",
    "                                     estimator_name=est_name, \n",
    "                                     feature_names=features, \n",
    "                                     method = 'sage', \n",
    "                                     normalize=False  \n",
    "                                       )\n",
    "\n",
    "    # Sum the SAGE values for each feature and then save results. \n",
    "    explainer.save(os.path.join(RESULTS_PATH, f'sage_{dataset}_{option}.nc'), sage_values)\n",
    "    #explainer.save(os.path.join(RESULTS_PATH, f'new_grouped_sage_rank_{dataset}_{option}.nc'), sage_rank)\n",
    "        \n",
    "DATASETS = ['new_severe_wind', 'lightning', 'road_surface']\n",
    "global_methods = [compute_sage_, compute_group_sage,]\n",
    "local_methods = [compute_shap]\n",
    "\n",
    "# TODO: perhaps make the global size as a fraction of the total dataset? \n",
    "# Implement a try and expect? \n",
    "\n",
    "GLOBAL_SIZE = 50000\n",
    "LOCAL_SIZE = 2500\n",
    "\n",
    "for dataset in DATASETS:\n",
    "    \n",
    "    # Load model and data.\n",
    "    model, X, y, groups = load_data_and_model(dataset, DATA_BASE_PATH, MODEL_BASE_PATH, \n",
    "                                     return_groups=True)\n",
    "    \n",
    "    # Subsample the dataset. \n",
    "    X_sub, y_sub = subsampler(X,y, GLOBAL_SIZE)\n",
    "     \n",
    "    X_local, y_local = subsampler(X_sub, y_sub, LOCAL_SIZE)\n",
    "    local_explainer = skexplain.ExplainToolkit(model, X_local, y_local)\n",
    "    for method in local_methods:\n",
    "        print(method)\n",
    "        method(local_explainer, dataset, option, est_name, X=X, model=model)\n",
    "\n",
    "    # Initialize the explainer. \n",
    "    explainer = skexplain.ExplainToolkit(model, X_sub, y_sub)    \n",
    "    for method in global_methods:\n",
    "        print(method)\n",
    "        method(explainer, dataset, option, est_name, X=X, model=model, groups=groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5c8060",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
