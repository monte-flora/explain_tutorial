{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca37fcf2",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://colab.research.google.com/github/monte-flora/explain_tutorial/blob/main/src/tutorial_notebooks/Notebook01_Generate_Explanations.ipynb)\n",
    "\n",
    "\n",
    "## Generate the Explainability Output\n",
    "\n",
    "In this notebook, we compute the explainability output used in the paper. The methods include:\n",
    "1. [Accumulated Local Effects (ALE)](https://christophm.github.io/interpretable-ml-book/ale.html)  \n",
    "2. [SHAP (Shapley Additive Explanations)](https://christophm.github.io/interpretable-ml-book/shap.html)\n",
    "3. [SAGE (Shapley Additive Global Explanations)](https://iancovert.com/blog/understanding-shap-sage/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72c1020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def using_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "    \n",
    "import sys, os \n",
    "from glob import glob\n",
    "if using_colab():\n",
    "    # When using Google Colab, need to clone the explain_tutorial repo\n",
    "    # Otherwise, the code assumes you are running these notebooks\n",
    "    # in their original directory structure. \n",
    "    try:\n",
    "        !git clone https://github.com/monte-flora/explain_tutorial\n",
    "    except:\n",
    "        print('explain_tutorial has already been cloned!')\n",
    "    sys.path.append('explain_tutorial')   \n",
    "else:\n",
    "    from os.path import dirname\n",
    "    path = dirname(dirname(os.getcwd()))\n",
    "    sys.path.append(path)\n",
    "\n",
    "from src.io.colab_io import GoogleDriveIO\n",
    "    \n",
    "# Download data from Google drive\n",
    "if using_colab():\n",
    "    downloader = GoogleDriveIO()\n",
    "    # Make a 'datasets' and 'models' directories\n",
    "    if not os.path.exists('datasets'):\n",
    "        os.mkdir('datasets')\n",
    "    if not os.path.exists('models'):\n",
    "        os.mkdir('models')\n",
    "    \n",
    "    # These are all the paths to files that were downloaded in Notebook00_Download_Data_and_Models\n",
    "    # If you are only interested in a single dataset, then you can remove the others\n",
    "    # which improves download times. \n",
    "    \n",
    "    paths_dict = {'lightning dataset' : '/content/datasets/lightning_dataset.csv', \n",
    "              'road_surface dataset' : '/content/datasets/road_surface_dataset.csv', \n",
    "              'severe_wind dataset' : '/content/datasets/severe_wind_dataset.csv', \n",
    "              'lightning model' : '/content/models/NN_classification.joblib', \n",
    "              'road_surface model' : '/content/models/JTTI_ProbSR_RandomForest.pkl', \n",
    "        'severe_wind model' : '/content/models/LogisticRegression_wind_severe_0km_None_first_hour_realtime.joblib', \n",
    "              }\n",
    "\n",
    "    for title in paths_dict.keys():\n",
    "        downloader.download(title, paths_dict[title])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1cd95c",
   "metadata": {},
   "source": [
    "#### Install different python packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c329e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neccesary packages to load the ML models from pickle\n",
    "%pip install scikit-explain==0.1.4 sage-importance imblearn daal4py scikit-learn==1.0.2 netCDF4 scikit-learn-intelex bayeshist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a401b35",
   "metadata": {},
   "source": [
    "#### Import python packages (internal and third party)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba4f8bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n"
     ]
    }
   ],
   "source": [
    "import skexplain \n",
    "from skexplain.common.importance_utils import to_skexplain_importance\n",
    "from src.io.io import load_data_and_model\n",
    "from src.common.util import subsampler, normalize_importance, compute_sage\n",
    "\n",
    "import pickle\n",
    "import shap\n",
    "import itertools\n",
    "import numpy as np\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d212d106",
   "metadata": {},
   "source": [
    "#### Setting the user constants (paths, parameters, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ba84d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants. \n",
    "N_BOOTSTRAP = 10\n",
    "N_BINS = 30\n",
    "# Colab only has 2 CPUs, so thats the default here. If you have access to more CPUs\n",
    "# Feel free to increase N_JOBS. The closer you are to number of features greatly decreases\n",
    "# the runtime. \n",
    "N_JOBS = 2\n",
    "GLOBAL_SIZE = 50000\n",
    "LOCAL_SIZE = 2500\n",
    "DATASET = 'road_surface'\n",
    "\n",
    "EVALUATION_FN = 'norm_aupdc'\n",
    "\n",
    "if not os.path.exists('results'):\n",
    "    os.mkdir('results')\n",
    "    \n",
    "BASE_PATH       = os.getcwd()\n",
    "RESULTS_PATH    = os.path.join(BASE_PATH, 'results')\n",
    "DATA_BASE_PATH  = os.path.join(BASE_PATH, 'datasets')\n",
    "MODEL_BASE_PATH = os.path.join(BASE_PATH, 'models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7a6bf9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function compute_sage_ at 0x7fe5bcb04b80>\n",
      "PermutationEstimator will use 4 jobs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22338a48da0e4e018fd9e52ac02c5999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function compute_group_sage at 0x7fe5c181e310>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ba77e1796f341348678fc7e2ead0ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute ALE \n",
    "def compute_ale(explainer, dataset, est_name, **kwargs): \n",
    "    ale = explainer.ale(features='all', n_bootstrap=N_BOOTSTRAP, n_bins=N_BINS, n_jobs=N_JOBS)\n",
    "    # Save ALE results (as netcdf file)\n",
    "    explainer.save(os.path.join(RESULTS_PATH, f'ale_{dataset}.nc'), ale, encoding=None)\n",
    "\n",
    "\n",
    "# Compute Shapely Additive Explanation (SHAP)\n",
    "def compute_shap(explainer, dataset, est_name, **kwargs):\n",
    "    X = kwargs['X']\n",
    "    features = kwargs['X'].columns\n",
    "    results = explainer.local_attributions('shap', \n",
    "                                       shap_kws={'masker' : \n",
    "                                      shap.maskers.Partition(X, max_samples=50, \n",
    "                                                             clustering=\"correlation\"), \n",
    "                                     'algorithm' : 'permutation'})\n",
    "\n",
    "\n",
    "    shap_rank = to_skexplain_importance(results[f'shap_values__{est_name}'].values, \n",
    "                                     estimator_name=est_name, \n",
    "                                     feature_names=features, \n",
    "                                     method ='shap_sum', \n",
    "                                     normalize=False    \n",
    "                                       )\n",
    "\n",
    "    # Sum the SHAP values for each feature and then save results. \n",
    "    explainer.save(os.path.join(RESULTS_PATH, f'shap_{dataset}.nc'), results, encoding=None)\n",
    "    explainer.save(os.path.join(RESULTS_PATH, f'shap_rank_{dataset}.nc'), shap_rank, encoding=None)\n",
    "\n",
    "# Compute SAGE\n",
    "def compute_sage_(explainer, dataset, est_name, **kwargs):\n",
    "    estimator = explainer.estimators[est_name]\n",
    "    \n",
    "    X = explainer.X\n",
    "    y = explainer.y\n",
    "    X_orig = kwargs['X']\n",
    "    \n",
    "    features = kwargs['X'].columns\n",
    "    sage_values = compute_sage(estimator, X.values, y, X_orig, n_jobs = N_JOBS)\n",
    "    sage_rank = to_skexplain_importance(sage_values,\n",
    "                                     estimator_name=est_name, \n",
    "                                     feature_names=features, \n",
    "                                     method = 'sage', \n",
    "                                     normalize=False  \n",
    "                                       )\n",
    "\n",
    "    # Sum the SAGE values for each feature and then save results. \n",
    "    explainer.save(os.path.join(RESULTS_PATH, f'sage_{dataset}.nc'), sage_rank, encoding=None)\n",
    "\n",
    "\n",
    "# Compute Grouped SAGE\n",
    "def compute_group_sage(explainer, dataset,  est_name, **kwargs):\n",
    "    \n",
    "    X = explainer.X\n",
    "    feature_groups = kwargs['groups']\n",
    "    # Group indices\n",
    "    groups = []\n",
    "    cols = list(X.columns)\n",
    "    features = []\n",
    "    for key, group in feature_groups.items():\n",
    "        ind_list = []\n",
    "        for feature in group:\n",
    "            ind_list.append(cols.index(feature))\n",
    "        groups.append(ind_list)\n",
    "        features.append(key)  \n",
    "    \n",
    "    estimator = explainer.estimators[est_name]\n",
    "    \n",
    "    y = explainer.y\n",
    "    X_orig = kwargs['X']\n",
    "    \n",
    "    sage_values = compute_sage(estimator, X.values, y, X_orig, groups=groups)\n",
    "    sage_rank = to_skexplain_importance(sage_values,\n",
    "                                     estimator_name=est_name, \n",
    "                                     feature_names=features, \n",
    "                                     method = 'sage', \n",
    "                                     normalize=False  \n",
    "                                       )\n",
    "\n",
    "    # Sum the SAGE values for each feature and then save results. \n",
    "    explainer.save(os.path.join(RESULTS_PATH, f'grouped_sage_{dataset}.nc'), sage_rank, encoding=None)\n",
    "    \n",
    "global_methods = [compute_ale, compute_sage_, compute_group_sage,]\n",
    "local_methods = [compute_shap]\n",
    "\n",
    "# Load model and data.\n",
    "model, X, y, groups = load_data_and_model(DATASET, DATA_BASE_PATH, MODEL_BASE_PATH, \n",
    "                                     return_groups=True)\n",
    "est_name = model[0]\n",
    "    \n",
    "# Subsample the dataset with GLOBAL_SIZE samples for the global methods. \n",
    "X_sub, y_sub = subsampler(X,y, GLOBAL_SIZE)\n",
    "\n",
    "# Initialize the explainer. \n",
    "global_explainer = skexplain.ExplainToolkit(model, X_sub, y_sub) \n",
    "    \n",
    "# Subsample the GLOBAL_SIZE samples with LOCAL_SIZE samples for the local methods\n",
    "X_local, y_local = subsampler(X_sub, y_sub, LOCAL_SIZE)\n",
    "local_explainer = skexplain.ExplainToolkit(model, X_local, y_local)\n",
    "    \n",
    "for method in local_methods:\n",
    "    print(method)\n",
    "    method(local_explainer, DATASET, est_name, X=X)\n",
    "\n",
    "for method in global_methods:\n",
    "    print(method)\n",
    "    method(global_explainer, DATASET, est_name, X=X, model=model, groups=groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4391618e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the explainability results to Google drive for Notebook02_Explainability_Tutorial.\n",
    "if using_colab():\n",
    "    uploader= GoogleDriveIO()\n",
    "    results_paths = glob('results/*')\n",
    "\n",
    "    for path in results_paths:\n",
    "        print(f'Uploading {path} dataset to Google Drive...')\n",
    "        uploader.upload(path, title=os.path.basename(path).replace('.nc', ''))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
