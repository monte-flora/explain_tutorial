{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca37fcf2",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://colab.research.google.com/github/monte-flora/explain_tutorial/blob/main/src/tutorial_notebooks/Notebook01_Generate_Explanations.ipynb)\n",
    "\n",
    "\n",
    "## Generate the Explainability Output\n",
    "\n",
    "In this notebook, we compute the explainability output used in the paper. The methods include:\n",
    "1. [Accumulated Local Effects (ALE)](https://christophm.github.io/interpretable-ml-book/ale.html)  \n",
    "2. [SHAP (Shapley Additive Explanations)](https://christophm.github.io/interpretable-ml-book/shap.html)\n",
    "3. [SAGE (Shapley Additive Global Explanations)](https://iancovert.com/blog/understanding-shap-sage/)\n",
    "\n",
    "For more details on the scikit-explain methods used here and more that we could not cover, there are more [tutorial notebooks on the github page](https://github.com/monte-flora/scikit-explain/tree/master/tutorial_notebooks)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d86bba",
   "metadata": {},
   "source": [
    "## A Machine Learning Explainability Tutorial for Atmospheric Sciences\n",
    "\n",
    "This article covers multiple topics in explainability, including definition of key terms. Some of the discussion are briefly reiterated here. \n",
    "\n",
    "\n",
    "\n",
    "1. **Interpretability versus Explainability**\n",
    "    - *Interpretability*: the degree to which an entire model and its components can be understood without additional methods.\n",
    "    - *Explainability*:  the degree to which any partially interpretable or uninterpretable model (i.e., black boxes) can be approximately understood through post hoc methods (e.g., verification, visualizations of important features, or learned relationships).\n",
    "    \n",
    "2. **Global versus Local Explainability**: \n",
    "    - *Global* : How does the trained model as a whole make predictions? How do components of the model affect the predictions?\n",
    "    - *Local*: Why did the model make a certain prediction for a specific set of examples?\n",
    "    \n",
    "    \n",
    "3. **Explainability method types**: \n",
    "    - *Feature Relevance and Feature Importance* : The ranking of features or sets of features by how much they contribute to a model’s output or its quality\n",
    "    - *Featue Effects* : The expected functional relationship between a feature (or set of features) and an ML model’s output\n",
    "    - *Feature Interactions*: How a given feature’s effect is dependent on other features and the strength of that effect \n",
    "    \n",
    "4. **Feature Relevance versus Importance**\n",
    "\n",
    "    - *Relevance* : How the input effects the model output.\n",
    "    - *Importance* : How the inputs effects the model performance. \n",
    "    \n",
    "5. **Model-Specific Importance versus Model-Agnostic Importance**\n",
    "\n",
    "    - *Model-Specific* : How important is this feature (or set of features) to this particular model, given this particular dataset? \n",
    "    \n",
    "    - *Model-Agnostic* : How important is this feature (or set of features) to any well-performing model, potentially regardless of the particular dataset. \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c6c5d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def using_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffd075ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "# Constants. \n",
    "N_BOOTSTRAP = 10\n",
    "N_BINS = 5\n",
    "# Colab only has 2 CPUs, so thats the default here. If you have access to more CPUs\n",
    "# Feel free to increase N_JOBS. The closer you are to number of features greatly decreases\n",
    "# the runtime. \n",
    "N_JOBS = 2\n",
    "GLOBAL_SIZE = 5000\n",
    "LOCAL_SIZE = 10\n",
    "EVALUATION_FN = 'norm_aupdc'\n",
    "##########################\n",
    "DATASET = 'road_surface'\n",
    "##########################\n",
    "\n",
    "if not os.path.exists('results'):\n",
    "    os.mkdir('results')\n",
    "\n",
    "if using_colab(): \n",
    "    BASE_PATH       = os.getcwd()\n",
    "else:\n",
    "    # If you are not using Goolge Colab, then you need to define \n",
    "    # your own base path where the explainability results will be stored. \n",
    "    BASE_PATH = '/work/mflora/explain_tutorial_results'\n",
    "    \n",
    "RESULTS_PATH    = os.path.join(BASE_PATH, 'results')\n",
    "DATA_BASE_PATH  = os.path.join(BASE_PATH, 'datasets')\n",
    "MODEL_BASE_PATH = os.path.join(BASE_PATH, 'models')\n",
    "\n",
    "if not os.path.exists(RESULTS_PATH):\n",
    "    os.makedirs(RESULTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d72c1020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os \n",
    "from glob import glob\n",
    "if using_colab():\n",
    "    # When using Google Colab, need to clone the explain_tutorial repo\n",
    "    # Otherwise, the code assumes you are running these notebooks\n",
    "    # in their original directory structure. \n",
    "    try:\n",
    "        !git clone https://github.com/monte-flora/explain_tutorial\n",
    "    except:\n",
    "        print('explain_tutorial has already been cloned!')\n",
    "    sys.path.append('explain_tutorial')   \n",
    "    from src.io.colab_io import GoogleDriveIO\n",
    "else:\n",
    "    from os.path import dirname\n",
    "    path = dirname(dirname(os.getcwd()))\n",
    "    sys.path.append(path)\n",
    "\n",
    "# Download data from Google drive\n",
    "if using_colab():\n",
    "    DATASET_PATHS = {'lightning' : '/content/datasets/lightning_dataset.csv', \n",
    "              'road_surface' : '/content/datasets/road_surface_dataset.csv', \n",
    "              'severe_wind' : '/content/datasets/severe_wind_dataset.csv', \n",
    "     }\n",
    "    MODEL_PATHS = {                  \n",
    "              'lightning' : '/content/models/NN_classification.joblib', \n",
    "              'road_surface' : '/content/models/JTTI_ProbSR_RandomForest.pkl', \n",
    "              'severe_wind' : '/content/models/LogisticRegression_wind_severe_0km_None_first_hour_realtime.joblib', \n",
    "              }\n",
    "\n",
    "    paths_dict = {f'{DATASET} dataset' :  DATASET_PATHS[DATASET], \n",
    "                  f'{DATASET} model' :  MODEL_PATHS[DATASET], \n",
    "                  }\n",
    "    \n",
    "    downloader = GoogleDriveIO()\n",
    "    # Make a 'datasets' and 'models' directories\n",
    "    if not os.path.exists('datasets'):\n",
    "        os.mkdir('datasets')\n",
    "    if not os.path.exists('models'):\n",
    "        os.mkdir('models')\n",
    "    \n",
    "    for title in paths_dict.keys():\n",
    "        downloader.download(title, paths_dict[title])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1cd95c",
   "metadata": {},
   "source": [
    "#### Install different python packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dc94ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-explain==0.1.4 in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (0.1.4)\n",
      "Requirement already satisfied: sage-importance in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (0.0.6)\n",
      "Requirement already satisfied: imblearn in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (0.0)\n",
      "Requirement already satisfied: daal4py in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (2024.6.0)\n",
      "Requirement already satisfied: scikit-learn==1.0.2 in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (1.0.2)\n",
      "Requirement already satisfied: netCDF4 in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (1.7.1.post2)\n",
      "Requirement already satisfied: scikit-learn-intelex in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (2024.6.0)\n",
      "Requirement already satisfied: bayeshist in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (0.2)\n",
      "Requirement already satisfied: numpy in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from scikit-explain==0.1.4) (1.22.4)\n",
      "Requirement already satisfied: pandas in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from scikit-explain==0.1.4) (1.5.0)\n",
      "Requirement already satisfied: matplotlib in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from scikit-explain==0.1.4) (3.8.4)\n",
      "Requirement already satisfied: shap>=0.30.0 in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from scikit-explain==0.1.4) (0.40.0)\n",
      "Requirement already satisfied: xarray>=0.16.0 in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from scikit-explain==0.1.4) (2023.12.0)\n",
      "Requirement already satisfied: tqdm in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from scikit-explain==0.1.4) (4.66.5)\n",
      "Requirement already satisfied: ipywidgets in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from scikit-explain==0.1.4) (8.1.5)\n",
      "Requirement already satisfied: statsmodels in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from scikit-explain==0.1.4) (0.14.2)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from scikit-explain==0.1.4) (0.13.2)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from scikit-learn==1.0.2) (1.11.4)\n",
      "Requirement already satisfied: joblib>=0.11 in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from scikit-learn==1.0.2) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from scikit-learn==1.0.2) (3.5.0)\n",
      "Requirement already satisfied: imbalanced-learn in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from imblearn) (0.12.3)\n",
      "Requirement already satisfied: daal==2024.6.0 in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from daal4py) (2024.6.0)\n",
      "Requirement already satisfied: tbb==2021.* in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from daal==2024.6.0->daal4py) (2021.13.1)\n",
      "Requirement already satisfied: cftime in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from netCDF4) (1.6.4)\n",
      "Requirement already satisfied: certifi in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from netCDF4) (2024.8.30)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from matplotlib->scikit-explain==0.1.4) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from matplotlib->scikit-explain==0.1.4) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from matplotlib->scikit-explain==0.1.4) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from matplotlib->scikit-explain==0.1.4) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from matplotlib->scikit-explain==0.1.4) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from matplotlib->scikit-explain==0.1.4) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from matplotlib->scikit-explain==0.1.4) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from matplotlib->scikit-explain==0.1.4) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from pandas->scikit-explain==0.1.4) (2024.1)\n",
      "Requirement already satisfied: slicer==0.0.7 in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from shap>=0.30.0->scikit-explain==0.1.4) (0.0.7)\n",
      "Requirement already satisfied: numba in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from shap>=0.30.0->scikit-explain==0.1.4) (0.55.2)\n",
      "Requirement already satisfied: cloudpickle in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from shap>=0.30.0->scikit-explain==0.1.4) (3.0.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from ipywidgets->scikit-explain==0.1.4) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from ipywidgets->scikit-explain==0.1.4) (8.27.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from ipywidgets->scikit-explain==0.1.4) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from ipywidgets->scikit-explain==0.1.4) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from ipywidgets->scikit-explain==0.1.4) (3.0.13)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from statsmodels->scikit-explain==0.1.4) (0.5.6)\n",
      "Requirement already satisfied: decorator in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets->scikit-explain==0.1.4) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets->scikit-explain==0.1.4) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets->scikit-explain==0.1.4) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets->scikit-explain==0.1.4) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets->scikit-explain==0.1.4) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets->scikit-explain==0.1.4) (0.6.2)\n",
      "Requirement already satisfied: exceptiongroup in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets->scikit-explain==0.1.4) (1.2.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets->scikit-explain==0.1.4) (4.12.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets->scikit-explain==0.1.4) (4.9.0)\n",
      "Requirement already satisfied: six in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from patsy>=0.5.6->statsmodels->scikit-explain==0.1.4) (1.16.0)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from numba->shap>=0.30.0->scikit-explain==0.1.4) (0.38.1)\n",
      "Requirement already satisfied: setuptools in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from numba->shap>=0.30.0->scikit-explain==0.1.4) (73.0.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets->scikit-explain==0.1.4) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets->scikit-explain==0.1.4) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets->scikit-explain==0.1.4) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets->scikit-explain==0.1.4) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets->scikit-explain==0.1.4) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /work/mflora/miniconda3/envs/skexplain/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets->scikit-explain==0.1.4) (0.2.3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Neccesary packages to load the ML models from pickle\n",
    "%pip install scikit-explain==0.1.4 sage-importance imblearn daal4py scikit-learn==1.0.2 netCDF4 scikit-learn-intelex bayeshist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a401b35",
   "metadata": {},
   "source": [
    "#### Import python packages (internal and third party)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba4f8bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skexplain \n",
    "from skexplain.common.importance_utils import to_skexplain_importance\n",
    "from src.io.io import load_data_and_model\n",
    "from src.common.util import subsampler, normalize_importance, compute_sage\n",
    "\n",
    "import pickle\n",
    "import shap\n",
    "import itertools\n",
    "import numpy as np\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d212d106",
   "metadata": {},
   "source": [
    "#### Setting the user constants (paths, parameters, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42a89d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function compute_shap at 0x14ca2e828af0>\n",
      "<function compute_ale at 0x14ca2e829000>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALE Numerical Features: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [01:59<00:00,  3.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function compute_sage_ at 0x14ca2e828b80>\n",
      "PermutationEstimator will use 2 jobs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "797cc9bc7daa41f382f8f2f8678c72a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function compute_group_sage at 0x14ca2e828c10>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "040b4fbd0eaa43c48581c6ab92838f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute ALE \n",
    "def compute_ale(explainer, dataset, est_name, **kwargs): \n",
    "    ale = explainer.ale(features='all', n_bootstrap=N_BOOTSTRAP, n_bins=N_BINS, n_jobs=N_JOBS)\n",
    "    # Save ALE results (as netcdf file)\n",
    "    explainer.save(os.path.join(RESULTS_PATH, f'ale_{dataset}.nc'), ale, encoding=None)\n",
    "\n",
    "\n",
    "# Compute Shapely Additive Explanation (SHAP)\n",
    "def compute_shap(explainer, dataset, est_name, **kwargs):\n",
    "    X = kwargs['X']\n",
    "    features = kwargs['X'].columns\n",
    "    results = explainer.local_attributions('shap', \n",
    "                                       shap_kws={'masker' : \n",
    "                                      shap.maskers.Partition(X, max_samples=50, \n",
    "                                                             clustering=\"correlation\"), \n",
    "                                     'algorithm' : 'permutation'})\n",
    "\n",
    "\n",
    "    shap_rank = to_skexplain_importance(results[f'shap_values__{est_name}'].values, \n",
    "                                     estimator_name=est_name, \n",
    "                                     feature_names=features, \n",
    "                                     method ='shap_sum', \n",
    "                                     normalize=False    \n",
    "                                       )\n",
    "\n",
    "    # Sum the SHAP values for each feature and then save results. \n",
    "    explainer.save(os.path.join(RESULTS_PATH, f'shap_{dataset}.nc'), results, encoding=None)\n",
    "    explainer.save(os.path.join(RESULTS_PATH, f'shap_rank_{dataset}.nc'), shap_rank, encoding=None)\n",
    "\n",
    "# Compute SAGE\n",
    "def compute_sage_(explainer, dataset, est_name, **kwargs):\n",
    "    estimator = explainer.estimators[est_name]\n",
    "    \n",
    "    X = explainer.X\n",
    "    y = explainer.y\n",
    "    X_orig = kwargs['X']\n",
    "    \n",
    "    features = kwargs['X'].columns\n",
    "    sage_values = compute_sage(estimator, X.values, y, X_orig, n_jobs = N_JOBS)\n",
    "    sage_rank = to_skexplain_importance(sage_values,\n",
    "                                     estimator_name=est_name, \n",
    "                                     feature_names=features, \n",
    "                                     method = 'sage', \n",
    "                                     normalize=False  \n",
    "                                       )\n",
    "\n",
    "    # Sum the SAGE values for each feature and then save results. \n",
    "    explainer.save(os.path.join(RESULTS_PATH, f'sage_{dataset}.nc'), sage_rank, encoding=None)\n",
    "\n",
    "\n",
    "# Compute Grouped SAGE\n",
    "def compute_group_sage(explainer, dataset,  est_name, **kwargs):\n",
    "    \n",
    "    X = explainer.X\n",
    "    feature_groups = kwargs['groups']\n",
    "    # Group indices\n",
    "    groups = []\n",
    "    cols = list(X.columns)\n",
    "    features = []\n",
    "    for key, group in feature_groups.items():\n",
    "        ind_list = []\n",
    "        for feature in group:\n",
    "            ind_list.append(cols.index(feature))\n",
    "        groups.append(ind_list)\n",
    "        features.append(key)  \n",
    "    \n",
    "    estimator = explainer.estimators[est_name]\n",
    "    \n",
    "    y = explainer.y\n",
    "    X_orig = kwargs['X']\n",
    "    \n",
    "    sage_values = compute_sage(estimator, X.values, y, X_orig, groups=groups)\n",
    "    sage_rank = to_skexplain_importance(sage_values,\n",
    "                                     estimator_name=est_name, \n",
    "                                     feature_names=features, \n",
    "                                     method = 'sage', \n",
    "                                     normalize=False  \n",
    "                                       )\n",
    "\n",
    "    # Sum the SAGE values for each feature and then save results. \n",
    "    explainer.save(os.path.join(RESULTS_PATH, f'grouped_sage_{dataset}.nc'), sage_rank, encoding=None)\n",
    "    \n",
    "global_methods = [compute_ale, compute_sage_, compute_group_sage,]\n",
    "local_methods = [compute_shap]\n",
    "\n",
    "# Load model and data.\n",
    "model, X, y, groups = load_data_and_model(DATASET, DATA_BASE_PATH, MODEL_BASE_PATH, \n",
    "                                     return_groups=True)\n",
    "est_name = model[0]\n",
    "    \n",
    "# Subsample the dataset with GLOBAL_SIZE samples for the global methods. \n",
    "X_sub, y_sub = subsampler(X,y, GLOBAL_SIZE)\n",
    "\n",
    "# Initialize the explainer. \n",
    "global_explainer = skexplain.ExplainToolkit(model, X_sub, y_sub) \n",
    "    \n",
    "# Subsample the GLOBAL_SIZE samples with LOCAL_SIZE samples for the local methods\n",
    "X_local, y_local = subsampler(X_sub, y_sub, LOCAL_SIZE)\n",
    "local_explainer = skexplain.ExplainToolkit(model, X_local, y_local)\n",
    "    \n",
    "for method in local_methods:\n",
    "    print(method)\n",
    "    method(local_explainer, DATASET, est_name, X=X)\n",
    "\n",
    "for method in global_methods:\n",
    "    print(method)\n",
    "    method(global_explainer, DATASET, est_name, X=X, model=model, groups=groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc99f485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the explainability results to Google drive for Notebook02_Explainability_Tutorial.\n",
    "if using_colab():\n",
    "    uploader= GoogleDriveIO()\n",
    "    results_paths = glob('results/*')\n",
    "\n",
    "    for path in results_paths:\n",
    "        print(f'Uploading {path} dataset to Google Drive...')\n",
    "        uploader.upload(path, title=os.path.basename(path).replace('.nc', ''))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
