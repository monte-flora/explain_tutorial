{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fd762b9",
   "metadata": {},
   "source": [
    "### Downloading the datasets and models \n",
    "\n",
    "The three datasets and trained ML models are provided at the following [link](https://doi.org/10.5281/zenodo.8136709). The following code downloads the dataset and unzips it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d777af",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>Warning: </b> Install zenodo_get before running the following command: *pip install zenodo_get*</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46e01e5",
   "metadata": {},
   "source": [
    "#### 1. Download the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7595d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Title: Weather-based ML Datasets\n",
      "Keywords: Severe weather; lightning; road conditions; satellite; convection-allowing; HRRR\n",
      "Publication date: 2023-07-11\n",
      "DOI: 10.5281/zenodo.8136709\n",
      "Total size: 446.1 MB\n",
      "\n",
      "Link: https://zenodo.org/api/files/9c909f91-63e8-4eb8-831d-f6af852f8ad5/datasets.zip   size: 446.1 MB\n",
      "datasets.zip is already downloaded correctly.\n",
      "All files have been downloaded.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "command = \"python -m zenodo_get https://doi.org/10.5281/zenodo.8184201\"   \n",
    "result = subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f251a5",
   "metadata": {},
   "source": [
    "#### 2. Unzip the data \n",
    "\n",
    "This step will unzip the datasets and models, which will create directory named \"datasets\" and \"models\" that  contain CSV-format version of each dataset and the corresponding models, respectively. This will unzip the data in the same location as where the data was downloaded. We recommend transfering the data and models to another location. You'll then use those data paths for the other notebooks to load the data and models. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64f96a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZIP file extracted successfully.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import zipfile\n",
    "\n",
    "# Path to the ZIP file\n",
    "zip_file_path = \"datasets.zip\"\n",
    "\n",
    "# Get the directory where the ZIP file is located\n",
    "extracted_dir = os.path.dirname(zip_file_path)\n",
    "\n",
    "# Open the ZIP file and extract its contents\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extracted_dir)\n",
    "\n",
    "print('ZIP file extracted successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c74415",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
